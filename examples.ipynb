{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why this notebook\n",
    "\n",
    "This notebook mainly serves the purpose to explain to you how to infer and evaluate your LLM outputs. \n",
    "\n",
    "The methods I am showing here are mostly from this blog: please read if you are interested how Google, HELM or Huggingface evaluate their LLMs (not regexing through the generated outputs).\n",
    "\n",
    "https://github.com/huggingface/blog/blob/main/evaluating-mmlu-leaderboard.md\n",
    "\n",
    "All the figures are from the blog as well! And to map to what I did, it is the HELM way and the\tAI Harness way.\n",
    "\n",
    "If there is any mistakes! Please let me know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shan/anaconda3/envs/env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:38<00:00, 49.02s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# I choose a instruction tuned mistral here because it follows instruction way better than llamas\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ehartford/dolphin-2.1-mistral-7b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"ehartford/dolphin-2.1-mistral-7b\") #teknium/CollectiveCognition-v1.1-Mistral-7B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, in this example, I am going to show you how to evaluate like the figure where you basically measuring how likely are these A,B,C,D are produced on your first immediate token right after your prompt.\n",
    "![Alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first generated token is: '\n",
      "' with a log probability of: -0.04539313539862633\n",
      "=====\n",
      "i love this house A: positive B: negative \n",
      "\n",
      "I\n",
      "The log probability of 'A' as the first generated token is: -7.283852577209473\n",
      "The log probability of 'B' as the first generated token is: -7.645366668701172\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Encode the prompt\n",
    "prompt = \"I love this house A: positive B: negative \\n\"\n",
    "text = \"\"\n",
    "inputs = tokenizer(prompt+text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate response with scores\n",
    "generate_args = {\n",
    "    \"max_length\": inputs.input_ids.size(1) + 2,  # Original length plus one token\n",
    "    \"output_scores\": True,\n",
    "    \"return_dict_in_generate\": True\n",
    "}\n",
    "\n",
    "# Generate one token to get its logprob\n",
    "generate_output = model.generate(**inputs, **generate_args)\n",
    "\n",
    "# Retrieve the scores for the generated token\n",
    "scores = generate_output.scores[0]  # scores for the first generated token\n",
    "\n",
    "# Convert the scores to probabilities\n",
    "log_probs = torch.nn.functional.log_softmax(scores, dim=-1)\n",
    "\n",
    "# Get the generated token ID\n",
    "generated_token_id = generate_output.sequences[0, inputs.input_ids.size(1)].item()  # First token after the input sequence\n",
    "\n",
    "# Get the log-probability of the generated token\n",
    "log_prob_of_generated_token = log_probs[0, generated_token_id].item()\n",
    "\n",
    "# Decode the generated token ID to the token string\n",
    "generated_token = tokenizer.decode(generated_token_id)\n",
    "\n",
    "print(f\"The first generated token is: '{generated_token}' with a log probability of: {log_prob_of_generated_token}\")\n",
    "print('=====')\n",
    "# Retrieve the full sequence of generated token IDs (including the prompt)\n",
    "generated_sequence_ids = generate_output.sequences[0]\n",
    "\n",
    "# Decode the entire generated sequence to a string\n",
    "print(tokenizer.decode(generated_sequence_ids, skip_special_tokens=True))\n",
    "\n",
    "# Get token IDs for 'A' and 'B'\n",
    "token_id_A = tokenizer.convert_tokens_to_ids('A')\n",
    "token_id_B = tokenizer.convert_tokens_to_ids('B')\n",
    "\n",
    "# Get the log-probabilities of 'A' and 'B'\n",
    "log_prob_of_A = log_probs[0, token_id_A].item()\n",
    "log_prob_of_B = log_probs[0, token_id_B].item()\n",
    "\n",
    "\n",
    "print(f\"The log probability of 'A' as the first generated token is: {log_prob_of_A}\")\n",
    "print(f\"The log probability of 'B' as the first generated token is: {log_prob_of_B}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the same thing, the only small tweak here is sum up the non-cap A,B,C,D as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shan/anaconda3/envs/env/lib/python3.9/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first generated token is: '\n",
      "' with a log probability of: -0.04539313539862633\n",
      "=====\n",
      "i love this house A: positive B: negative \n",
      "\n",
      "I\n",
      "The log probability of 'A' as the first generated token is: -14.71200180053711\n",
      "The log probability of 'B' as the first generated token is: -16.32606792449951\n"
     ]
    }
   ],
   "source": [
    "# Encode the prompt\n",
    "prompt = \"i love this house A: positive B: negative \\n\"\n",
    "text = \"\"\n",
    "inputs = tokenizer(prompt+text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate response with scores\n",
    "generate_args = {\n",
    "    \"max_length\": inputs.input_ids.size(1) + 2,  # Original length plus one token\n",
    "    \"output_scores\": True,\n",
    "    \"return_dict_in_generate\": True\n",
    "}\n",
    "\n",
    "# Generate one token to get its logprob\n",
    "generate_output = model.generate(**inputs, **generate_args)\n",
    "\n",
    "# Retrieve the scores for the generated token\n",
    "scores = generate_output.scores[0]  # scores for the first generated token\n",
    "\n",
    "# Convert the scores to probabilities\n",
    "log_probs = torch.nn.functional.log_softmax(scores, dim=-1)\n",
    "\n",
    "# Get the generated token ID\n",
    "generated_token_id = generate_output.sequences[0, inputs.input_ids.size(1)].item()  # First token after the input sequence\n",
    "\n",
    "# Get the log-probability of the generated token\n",
    "log_prob_of_generated_token = log_probs[0, generated_token_id].item()\n",
    "\n",
    "# Decode the generated token ID to the token string\n",
    "generated_token = tokenizer.decode(generated_token_id)\n",
    "\n",
    "print(f\"The first generated token is: '{generated_token}' with a log probability of: {log_prob_of_generated_token}\")\n",
    "print('=====')\n",
    "# Retrieve the full sequence of generated token IDs (including the prompt)\n",
    "generated_sequence_ids = generate_output.sequences[0]\n",
    "\n",
    "# Decode the entire generated sequence to a string\n",
    "print(tokenizer.decode(generated_sequence_ids, skip_special_tokens=True))\n",
    "\n",
    "# Get token IDs for 'A' and 'B'\n",
    "token_id_A = tokenizer.convert_tokens_to_ids('A')\n",
    "token_id_B = tokenizer.convert_tokens_to_ids('B')\n",
    "\n",
    "token_id_a = tokenizer.convert_tokens_to_ids('a')\n",
    "token_id_b = tokenizer.convert_tokens_to_ids('b')\n",
    "\n",
    "# Get the log-probabilities of 'A' and 'B'\n",
    "log_prob_of_A = log_probs[0, token_id_A].item()\n",
    "log_prob_of_B = log_probs[0, token_id_B].item()\n",
    "\n",
    "# Print the log-probabilities of 'A' and 'B'\n",
    "log_prob_of_a = log_probs[0, token_id_a].item()\n",
    "log_prob_of_b = log_probs[0, token_id_b].item()\n",
    "\n",
    "log_prob_of_A = log_prob_of_A+log_prob_of_a\n",
    "log_prob_of_B = log_prob_of_B+log_prob_of_b\n",
    "\n",
    "print(f\"The log probability of 'A' as the first generated token is: {log_prob_of_A}\")\n",
    "print(f\"The log probability of 'B' as the first generated token is: {log_prob_of_B}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small demo to see whether instruction helps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first generated token is: '\n",
      "' with a log probability of: -0.41807690262794495\n",
      "=====\n",
      "i love this house A: positive B: negative \n",
      "Output only A or B please:\n",
      "\n",
      "A\n",
      "The log probability of 'A' as the first generated token is: -7.696923136711121\n",
      "The log probability of 'B' as the first generated token is: -11.69415807723999\n"
     ]
    }
   ],
   "source": [
    "# Encode the prompt\n",
    "prompt = \"i love this house A: positive B: negative \\nOutput only A or B please:\\n\"\n",
    "text = \"\"\n",
    "inputs = tokenizer(prompt+text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate response with scores\n",
    "generate_args = {\n",
    "    \"max_length\": inputs.input_ids.size(1) + 2,  # Original length plus one token\n",
    "    \"output_scores\": True,\n",
    "    \"return_dict_in_generate\": True\n",
    "}\n",
    "\n",
    "# Generate one token to get its logprob\n",
    "generate_output = model.generate(**inputs, **generate_args)\n",
    "\n",
    "# Retrieve the scores for the generated token\n",
    "scores = generate_output.scores[0]  # scores for the first generated token\n",
    "\n",
    "# Convert the scores to probabilities\n",
    "log_probs = torch.nn.functional.log_softmax(scores, dim=-1)\n",
    "\n",
    "# Get the generated token ID\n",
    "generated_token_id = generate_output.sequences[0, inputs.input_ids.size(1)].item()  # First token after the input sequence\n",
    "\n",
    "# Get the log-probability of the generated token\n",
    "log_prob_of_generated_token = log_probs[0, generated_token_id].item()\n",
    "\n",
    "# Decode the generated token ID to the token string\n",
    "generated_token = tokenizer.decode(generated_token_id)\n",
    "\n",
    "print(f\"The first generated token is: '{generated_token}' with a log probability of: {log_prob_of_generated_token}\")\n",
    "print('=====')\n",
    "# Retrieve the full sequence of generated token IDs (including the prompt)\n",
    "generated_sequence_ids = generate_output.sequences[0]\n",
    "\n",
    "# Decode the entire generated sequence to a string\n",
    "print(tokenizer.decode(generated_sequence_ids, skip_special_tokens=True))\n",
    "\n",
    "# Get token IDs for 'A' and 'B'\n",
    "token_id_A = tokenizer.convert_tokens_to_ids('A')\n",
    "token_id_B = tokenizer.convert_tokens_to_ids('B')\n",
    "\n",
    "token_id_a = tokenizer.convert_tokens_to_ids('a')\n",
    "token_id_b = tokenizer.convert_tokens_to_ids('b')\n",
    "\n",
    "# Get the log-probabilities of 'A' and 'B'\n",
    "log_prob_of_A = log_probs[0, token_id_A].item()\n",
    "log_prob_of_B = log_probs[0, token_id_B].item()\n",
    "\n",
    "# Print the log-probabilities of 'A' and 'B'\n",
    "log_prob_of_a = log_probs[0, token_id_a].item()\n",
    "log_prob_of_b = log_probs[0, token_id_b].item()\n",
    "\n",
    "log_prob_of_A = log_prob_of_A+log_prob_of_a\n",
    "log_prob_of_B = log_prob_of_B+log_prob_of_b\n",
    "\n",
    "print(f\"The log probability of 'A' as the first generated token is: {log_prob_of_A}\")\n",
    "print(f\"The log probability of 'B' as the first generated token is: {log_prob_of_B}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the prob and answer do changes as we requested :\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is an example on how to extract the last token, and the second last token as an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for the last token: tensor([ 0.9806, -2.3296,  3.0921,  ...,  6.5378,  0.3018,  3.7267],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Embedding for the second-to-last token: tensor([-2.2555,  2.6335,  5.9585,  ...,  4.3784,  6.0477,  1.9409],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Encode the prompt\n",
    "prompt = \"i love this house A: positive B: negative \\n\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate response with scores\n",
    "generate_args = {\n",
    "    \"max_length\": inputs.input_ids.size(1) + 2,  # Original length plus one token\n",
    "    \"output_scores\": True,\n",
    "    \"return_dict_in_generate\": True\n",
    "}\n",
    "\n",
    "# Generate one token to get its logprob\n",
    "generate_output = model.generate(**inputs, **generate_args)\n",
    "\n",
    "# Retrieve the full sequence of generated token IDs (including the prompt)\n",
    "generated_sequence_ids = generate_output.sequences[0]\n",
    "\n",
    "# Run the model again with the generated sequence to get hidden states\n",
    "outputs = model(generated_sequence_ids.unsqueeze(0), output_hidden_states=True)\n",
    "\n",
    "# Retrieve the last hidden state\n",
    "hidden_states = outputs.hidden_states\n",
    "last_hidden_state = hidden_states[-1]\n",
    "\n",
    "# Get the embeddings for the last and second-to-last tokens\n",
    "# -1 is the last token, -2 is the second-to-last token in the sequence\n",
    "last_token_embedding = last_hidden_state[0, -1, :]\n",
    "second_last_token_embedding = last_hidden_state[0, -2, :]\n",
    "\n",
    "print(\"Embedding for the last token:\", last_token_embedding)\n",
    "print(\"Embedding for the second-to-last token:\", second_last_token_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here:\n",
    "\n",
    "We are basically feeding all the tokens (token_ids) prompt+generated back to the model;\n",
    "\n",
    "And we are kinda asking how to likely we generate these tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GreedySearchDecoderOnlyOutput(sequences=tensor([[    1,   613,  2016,   456,  2134,   330, 28747,  5278,   365, 28747,\n",
       "          7087, 28705,    13,    13, 28737]]), scores=(tensor([[-12.0531, -11.6898,  -0.6606,  ...,  -4.7127,   4.2573, -12.7862]]), tensor([[-11.3678, -11.5735,  -0.9566,  ...,  -3.5125,   3.9774, -10.2945]])), attentions=None, hidden_states=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i love this house A: positive B: negative \\n\\nI'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(generate_output.sequences[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore it drives us to the last method, directly giving answers and just evaluate the whole seq generation likelihood.\n",
    "\n",
    "![Alt text](image-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most likely sequence is sequence number 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# # Initialize your model and tokenizer\n",
    "# model = AutoModelForCausalLM.from_pretrained('gpt2')  # Replace with your model\n",
    "# tokenizer = AutoTokenizer.from_pretrained('gpt2')  # Replace with your tokenizer\n",
    "\n",
    "# Text sequences\n",
    "text_seq1 = 'i love this house A: positive B: negative \\nA'\n",
    "text_seq2 = 'i love this house A: positive B: negative \\nB'\n",
    "\n",
    "sequences = [text_seq1, text_seq2]\n",
    "\n",
    "def calculate_log_likelihood(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    return outputs.loss.item()\n",
    "\n",
    "# Calculate log likelihood for each sequence\n",
    "log_likelihoods = [calculate_log_likelihood(model, tokenizer, seq) for seq in sequences]\n",
    "\n",
    "# Determine which sequence has the highest likelihood\n",
    "most_likely_sequence_index = log_likelihoods.index(min(log_likelihoods))  # Lower loss means higher likelihood\n",
    "most_likely_sequence = sequences[most_likely_sequence_index]\n",
    "\n",
    "print(f\"The most likely sequence is sequence number {most_likely_sequence_index + 1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So overall some notes for Shan(myself) and others:\n",
    "\n",
    "1. evaluating the seq generation prob != this seq is the most probable one the model will generate\n",
    "\n",
    "    1.1 from my limited experience, I do see many models do not follow instructions or do not generate the right thing even they are high on LLM leaderboard \n",
    "2. notice the difference in performance here (source from https://github.com/huggingface/blog/blob/main/evaluating-mmlu-leaderboard.md)\n",
    "    2.1 do we at the end of the day need a powerful LM to be resolver to actually evaluate them?\n",
    "\n",
    "\n",
    "<div>\n",
    "<table><p>\n",
    "  <tbody>\n",
    " <tr style=\"text-align: left;\">\n",
    "  <td>Original implementation</td>\n",
    "  <td>HELM</td>\n",
    "  <td>AI Harness (as of Jan 2023)</td>\n",
    " </tr>\n",
    "  <tr style=\" vertical-align: top;\">\n",
    "    <td> We compare the probabilities of the following letter answers:\n",
    "</td>\n",
    "    <td>The model is expected to generate as text the following letter answer:\n",
    "</td>\n",
    "    <td>We compare the probabilities of the following full answers:\n",
    "</td>\n",
    "  </tr>\n",
    "  <tr style=\" vertical-align: top;\">\n",
    "    <td>  A <br>\n",
    " B <br>\n",
    " C <br>\n",
    " D\n",
    "</td>\n",
    "    <td>A\n",
    "</td>\n",
    "    <td> A. It damaged support for the US model of political economy and capitalism <br>\n",
    " B. It created anger at the United States for exaggerating the crisis <br>\n",
    " C. It increased support for American global leadership under President Obama <br>\n",
    " D. It reduced global use of the US dollar\n",
    "</td>\n",
    "  </tr>\n",
    "  </tbody>\n",
    "</table><p>\n",
    "</div>\n",
    "\n",
    "|                                           | MMLU (HELM) | MMLU (Harness) | MMLU (Original) |\n",
    "|:------------------------------------------|------------:|---------------:|----------------:|\n",
    "| llama-65b                     |       **0.637** |          0.488 |           **0.636** |\n",
    "| tiiuae/falcon-40b                         |       0.571 |          **0.527** |           0.558 |\n",
    "| llama-30b                     |       0.583 |          0.457 |           0.584 |\n",
    "| EleutherAI/gpt-neox-20b                   |       0.256 |          0.333 |           0.262 |\n",
    "| llama-13b                     |       0.471 |          0.377 |           0.47  |\n",
    "| llama-7b                      |       0.339 |          0.342 |           0.351 |\n",
    "| tiiuae/falcon-7b                          |       0.278 |          0.35  |           0.254 |\n",
    "| togethercomputer/RedPajama-INCITE-7B-Base |       0.275 |          0.34  |           0.269 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
